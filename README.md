# MachineLearning Perceptron
 Machine Learning techniques to implement Single-Layer and Multi-Layer Perceptron models.

 
# Aim
The goal of this assignment is to read, understand, implement, and evaluate the machine learning algorithms called Perceptron and Multi-Layer Perceptron.

# Theory/Working
Neural Networks in Machine Learning topic is a varied topic, but it stems from the simple idea of the nervous system in the biological context, where the brain sends the input to a muscle or an organ through something called ‘neurons’ which are responsible for the processing of the input and sending it to the right organ/muscle. In a simple neural network system in Machine Learning, the user sends an input, and the nodes (like neurons) are responsible for calculations and output predictions given training on historical data and their labels. A neural network link that contains computations to track features and uses Artificial Intelligence in the input data is known as Perceptron. This neural links to the artificial neurons using simple logic gates with binary outputs. An artificial neuron invokes the mathematical function and has node, input, weights, and output equivalent to the cell nucleus, dendrites, synapse, and axon, respectively, compared to a biological neuron.

There are two types of Perceptron used in this assignment:
1. Single Layer Perceptron:
A single-layer Perceptron is a binary classification algorithm capable only of learning linearly separable patterns. As the name suggests, they are a type of supervised learning algorithm which contains only one layer containing the net input function and then an activation function that predicts the new data using the processing power and training from the old data. Some of the jargon around single-layer perceptron are:
    1. Input Layers: The input layer in Perceptron is made of features in the dataset taken into the system for further processing. The number of neurons is equal to the number of features in the dataset
    2. Weights: They determine the importance of a feature in the whole dataset i.e., the higher the number, the higher the importance of a feature. The single-layer perceptron doesn’t have prior knowledge, so the initial weights are assigned randomly and then adjusted to minimize errors using the perceptron learning algorithm while predicting labels for new data.
    3. Bias: It is the same as intercept in linear equations, and is an additional parameter used to modify the output to give correct predictions after the training along with the weighted sum of the inputs. We will initialize the bias = 0.
    4. Net Sum: Calculates the total sum of the product of the weights and the inputs.
    5. Activation Function: A neuron can be activated (1) or not (0) and the activation function is the determiner. The weighted sums and bias are compared with the activation function to give the predictions. We will use the sigmoid function as our activation function. The sigmoid function is used as it outputs probabilities between zero and one which is helpful for binary classification tasks as inputs spread over a large range of values will be reduced to probabilities between zero and one thus making it easy to set a threshold value for classification. The input to the activation function is the net sum plus the bias .
    6. Output: the output of a single layer perceptron is calculated by the equation y= activation_function(w.x +b)
    7. Error: The error in the single-layer perceptron is calculated by the difference between the desired output and the actual output.
    8. Architecture of the required neural network: The number of input units is 9 corresponding to the number of independent variables. <br> <br>
The number of weights is 9 corresponding to the number of input units, the activation function used is sigmoid. the output is the dependent variable “fire” which has two values “yes” or “no” thus making the given problem a binary classification problem. The threshold is set equal to the bias.<br><br>
We set a learning rate and iteration rate when initializing an object of the Perceptron class. The learning rate is the adjustment value which adjusts the error by modifying the weights and bias by additions or subtractions. To get the best accuracy score, we set an iteration rate that helps to adjust the weights during those iterations. It’s not always ideal to give a really big number but experimenting with the iteration value can help in a good scoring of the weights and bias for predictions.<br><br>
<i>Limitations:</i> A single-layer perceptron can only implement linearly separable classes. It can execute all logic gates except the XOR gate as the classes aren’t linearly separable. Thus, this limitation provides the need for a multi-layer perceptron, though it cannot guarantee the best results in every situation, it tries to calculate with many layers which increases the chances of having a better model.

 2. Multi-Layer Perceptron:
A multi-layer Perceptron is a neural network that is a feedforward neural network as the inputs and the weights are multiplied together and the weighted sum is input to the activation function along with the bias. a multilayer perceptron has input layers corresponding to the number of features and output layers corresponding to the dependent variable, but it has also had a number of hidden layers where each hidden layer consists of one or more neurons.
    1. Input layer: The input layer in Perceptron is made of features in the dataset taken into the system for further processing. The number of neurons is equal to the number of features in the dataset.
    2. Hidden layers: The hidden layers have one or more neurons stacked inside them. The hidden layers can choose any activation function for the weighted sum. The ith hidden layer output is fed into the (i+1)th hidden layer as an input with the respective weights and bias. The first hidden layer has an input of a weighted sum of features and initial weights along with the bias.
    3. Output layer: The output layer is corresponding to the number of dependent variables in the dataset. the output of the output layer is the final output of the multi-layer perceptron network. Unlike the hidden layers, the output layer cannot choose any arbitrary function as the choice of function is determined by the problem which is being solved. For a binary classification problem, we need a function that has output between range zero and one, the function for the hidden layer and output layer is chosen as sigmoid.
    4. Activation functions: We will use the sigmoid function as our activation function. The sigmoid function is used as it outputs probabilities between zero and one which is helpful for binary classification tasks as inputs spread over a large range of values will be reduced to probabilities between zero and one thus making it easy to set a threshold value for classification. The input to the activation function is the net sum plus the bias
    5. Loss function: The loss function is the difference between the current output and the actual output. The goal of training a model is to reduce the loss function (mean square error).
    6. Backpropagation: The algorithm to calculate the gradient vector to adjust the weights and biases is called backpropagation which computes the partial derivates of the cost function with respect to the weights.
    7. Gradient descent: The gradient descent algorithm is used to find the minimum of the cost function by computing the gradient and taking small steps along the negative of the gradient.
    8. Weights: They determine the importance of a feature in the whole dataset i.e., the higher the number, the higher the importance of a feature. The weights for the input layer and the weights for the hidden layers are also set randomly but the difference is the input layer weights are multiplied by the inputs and the hidden layer weights are multiplied by the output of the previously hidden layer.<br><br>
The input layer consists of the number of features in the dataset. the output layers consist of the number of the dependent variable so for the multilayer perceptron the number of neurons in the input layer is 9 and the output layer is one. The first hidden layer is stacked with 3 neurons which get an input of the weighted sum i.e., inputs *weights plus bias, and the output is then given by an activation function which in the given case is sigmoid thus the output is then feedforward as input to the next neurons multiplied by the weights plus bias for the respective layer. The output of the last hidden layer is then fed into the output layer as an input multiplied by the weights which is fed into the activation function which is sigmoid to give an actual output.<br><br>
The reason for multi-layer perceptron should have multiple activation functions is to give continuous output to compute the gradient for the gradient descent algorithm for updating the weights. The error for the multilayer perceptron is used by calculating the cost function which for the given problem is a mean squared error that computes the square of the difference between the expected and actual outputs for a single neuron. The final cost function is the mean of the cost functions of each neuron is given by the formula
$$MSE = \dfrac{1}{n} * \Sigma(actual – forecast)^2$$<br>
To find the minimum value of the cost function the gradient of a function will give the direction of the steepest ascent. The negative gradient will give the steepest descent. The length of the gradient will give an indication of the slope. The algorithm to minimize the function is to compute the gradient of the function and take small step sizes to reduce the cost function is called backpropagation.<br><br>
The process of continuously taking small steps along the negative gradient is called gradient descent. Backpropagation relies on the chain rule which is for each neuron it calculates the sensitivity of the cost function with respect to a random weight w which is then equal to the sensitivity of the linear output with respect to the weight multiplied by the sensitivity of y with respect to w is also calculated where y is the output of the activation function and for each output y the sensitivity of the cost function with respect to y.<br><br>
The sensitivity of the cost function with respect to the bias is identical where we replace the sensitivity of the cost function with respect to random bias b and the sensitivity of y with respect to b which is a bias.<br><br>
Since the final cost function is the summation of the cost function of each layer the final sensitivity of the cost function with the weights will also be a summation of the sensitivity of the cost function of all neurons with respect to the weights and the same goes for the sensitivity of the cost function with respect to biases. Thus, the cost function is indirectly sensitive to the activation function of a neuron which introduces the idea of backpropagation. To reduce the cost function, we backpropagate i.e., calculate the influences of the $n-1^th$ hidden layer on the nth layer and to get the desired outputs the weights are changed such that the output neuron matches the desired and actual outputs. Thus the weight changes are proportional to the larger activation values i.e. based on the idea of neurons that wire together fire together and the ones which are to be desired to be active. The sensitivity of a given variable with respect to another variable is calculated as the partial derivative of the given variable with respect to the second variable.<br><br>


3. Conclusion:
By conducting experimental test cases and tuning different parameters, we come to the conclusion that even though Multi-Layer Perception has higher capabilities, it may not always be the best use for you, it may sometime be defeated by the simple Single Layer Perceptron. Overall, Multi-Layer Perceptron has one of the best ideologies to make a machine learn and function on its own, and both the Perceptron worked better than the standard Logistic Regression function of the Scikit-Learn library meaning that the learning feature and weight adjustment feature of the Perceptron makes it robust and tuneable for each and every type of data and can be well suited to be used as an effective classifier to solve, even though it carries the risk of high cost of computation power and time consumption.
       
